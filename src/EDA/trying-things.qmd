--------------------------------- (rough notes ~ ignore file) ---------------------------------------

\subsubsection{[EDA] Sentiment Distributions}

```{r}
# Load necessary packages
library(dplyr)
library(tidytext)
library(ggplot2)
library(textdata)
library(data.table)

# Assume 'tweet' is your data.table with ~2.8 million rows
# Ensure 'tweet' has columns: tweet_id, author_id, inbound, text

# Convert to tibble for tidy operations (if not already)
tweet_tbl <- as_tibble(tweet)

# Get AFINN lexicon
afinn <- get_sentiments("afinn")

# Identify top brands if you want to focus on a subset (optional)
top_brands <- tweet_tbl %>%
  group_by(author_id) %>%
  dplyr::summarise(count = dplyr::n()) %>%
  top_n(3, count) %>%
  pull(author_id)

# Filter to just top brands for demonstration (optional)
tweet_tbl <- tweet_tbl %>% filter(author_id %in% top_brands)
```

```{r}
# Check how many rows total
nrow(tweet_tbl)
# Ensure chunk_size and splitting logic is correct:
chunk_size <- 500000
tweet_chunks <- split(tweet_tbl, ceiling(seq_len(nrow(tweet_tbl))/chunk_size))
length(tweet_chunks) # Should be > 1 if you have > 500000 rows
```

```{r}
# Define a function to process each chunk:
process_chunk <- function(df_chunk, lexicon) {
  
  # df_chunk should have author_id
  #str(df_chunk) # Check this for debugging

  df_words <- df_chunk %>%
    select(tweet_id, author_id, inbound, text) %>%
    unnest_tokens(word, text, token = "words")

  # Check here too
  #str(df_words) # Should show author_id

  df_sentiment <- df_words %>%
    inner_join(lexicon, by = "word") %>%
    group_by(tweet_id, author_id, inbound) %>%
    dplyr::summarise(sentiment = mean(value), .groups = "drop")

  # Check the structure after summarise
  str(df_sentiment) # Should show tweet_id, author_id, inbound, sentiment

  return(df_sentiment)
}
```

```{r}
# Process each chunk and store results
results_list <- lapply(tweet_chunks, process_chunk, lexicon = afinn)
```

```{r}
# Combine all chunk results
tweet_sentiment_all <- bind_rows(results_list)

# At this point, 'tweet_sentiment_all' has sentiment scores per tweet
# Plot sentiment distributions by brand
ggplot(data = tweet_sentiment_all, aes(x = sentiment, fill = author_id)) +
  geom_histogram(binwidth = 0.1, alpha = 0.7, color = "black") +
  facet_wrap(~ author_id, scales = "free_y") +
  theme_bw() +
  scale_fill_brewer(palette = "Set1") +
  geom_vline(xintercept = 0, color = "coral", size = 1.5, alpha = 0.6, linetype = "longdash") +
  labs(
    title = "Sentiment Distributions by Brand (AFINN Lexicon, Chunked)",
    x = "Sentiment Score",
    y = "Count"
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

```{r}
# Plot sentiment distribution for inbound tweets
inbound_tweets <- tweet %>%
  filter(inbound == TRUE)

ggplot(inbound_tweets, aes(x = sentiment_score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "white", alpha = 0.7) +
  labs(
    title = "Sentiment Distribution for Inbound Tweets",
    x = "Sentiment Score",
    y = "Frequency"
  ) +
  theme_minimal()


```

```{r}
# Filter for top 3 brands
top3_brands <- c("AmazonHelp", "AppleSupport", "Uber_Support")
top3 <- tweet %>%
  filter(author_id %in% top3_brands)


# Compute follow-up volume
followup_volume <- top3 %>%
  group_by(in_response_to_tweet_id) %>%
  dplyr::summarise(follow_up_count = n()) %>%
  filter(!is.na(in_response_to_tweet_id))

# Merge back to the top 3 data
top3 <- left_join(top3, followup_volume, by = c("tweet_id" = "in_response_to_tweet_id"))
top3$follow_up_count[is.na(top3$follow_up_count)] <- 0

# Plot follow-up volume for top 3 brands
ggplot(top3, aes(x = follow_up_count, fill = author_id)) +
  geom_histogram(binwidth = 1, alpha = 0.7, color = "black") +
  facet_wrap(~ author_id, scales = "free_y") +
  theme_bw() +
  labs(
    title = "Follow-Up Volume Distributions for Top 3 Brands",
    x = "Follow-Up Count",
    y = "Frequency"
  )

```

```{r}
# Ensure both datasets are data.tables for efficient merging
library(data.table)
setDT(tweet_sentiment_all)
setDT(top3)

# Merge sentiment scores into top3
top3 <- merge(
  top3,
  tweet_sentiment_all[, .(tweet_id, sentiment)], # Select only necessary columns
  by = "tweet_id",
  all.x = TRUE # Keep all rows from top3 even if there's no match in tweet_sentiment_all
)

# Handle any NA sentiment values (if there are unmatched tweet_ids)
top3$sentiment[is.na(top3$sentiment)] <- 0

# Check the structure of the updated top3
str(top3)

```

```{r}
# Compute follow-up volume
followup_volume <- top3 %>%
  group_by(in_response_to_tweet_id) %>%
  dplyr::summarise(follow_up_count = n()) %>%
  filter(!is.na(in_response_to_tweet_id))

# Merge back to the top 3 data
top3 <- left_join(top3, followup_volume, by = c("tweet_id" = "in_response_to_tweet_id"))
top3$follow_up_count[is.na(top3$follow_up_count)] <- 0

# Plot follow-up volume for top 3 brands
ggplot(top3, aes(x = follow_up_count, fill = author_id)) +
  geom_histogram(binwidth = 1, alpha = 0.7, color = "black") +
  facet_wrap(~ author_id, scales = "free_y") +
  theme_bw() +
  labs(
    title = "Follow-Up Volume Distributions for Top 3 Brands",
    x = "Follow-Up Count",
    y = "Frequency"
  )

```

```{r}
# Load required packages
library(dplyr)
library(forcats)
library(lsr)      # for cramersV
library(knitr)    # for kable

# Define the top three brands
top_brands <- c("AmazonHelp", "AppleSupport", "Uber_Support")

# Filter to top 3 brands
top3_companies <- tweets %>%
  filter(author_id %in% top_brands)

# Convert to appropriate factor types
top3_companies <- top3_companies %>%
  mutate(
    author_id = factor(author_id, levels = top_brands),
    inbound = factor(inbound, levels = c(FALSE, TRUE), labels = c("FALSE", "TRUE")),
    sentiment_category = factor(sentiment_category, levels = c("Negative","Neutral","Positive"))
  )

# Create a follow-up binary variable:
# If response_tweet_id is empty or NA, no follow-up; otherwise has follow-up.
top3_companies <- top3_companies %>%
  mutate(
    follow_up_binary = ifelse(response_tweet_id == "" | is.na(response_tweet_id), 
                              "No_Followup", "Has_Followup")
  ) %>%
  mutate(follow_up_binary = factor(follow_up_binary, levels = c("No_Followup","Has_Followup")))

# TEST 1: Brand vs. Sentiment Category
brand_sent_table <- table(top3_companies$author_id, top3_companies$sentiment_category)
brand_sent_chi <- chisq.test(brand_sent_table)
brand_sent_cramer <- cramersV(brand_sent_table)

# TEST 2: Brand vs. Follow-Up Presence
brand_followup_table <- table(top3_companies$author_id, top3_companies$follow_up_binary)
brand_followup_chi <- chisq.test(brand_followup_table)
brand_followup_cramer <- cramersV(brand_followup_table)

# TEST 3: Inbound vs. Follow-Up Presence
inbound_followup_table <- table(top3_companies$inbound, top3_companies$follow_up_binary)
inbound_followup_chi <- chisq.test(inbound_followup_table)
inbound_followup_cramer <- cramersV(inbound_followup_table)

# Extract results into a data frame
# Replace the placeholders below with the actual p-values and Cramér’s V after running the code.
chi_results <- data.frame(
  Test = c("Brand vs Sentiment Category",
           "Brand vs Follow-Up Presence",
           "Inbound vs Follow-Up Presence"),
  `Chi-Squared P-Value` = c(brand_sent_chi$p.value,
                            brand_followup_chi$p.value,
                            inbound_followup_chi$p.value),
  `Cramér's V` = c(brand_sent_cramer, brand_followup_cramer, inbound_followup_cramer),
  stringsAsFactors = FALSE
)

# Format p-values for readability
chi_results$`Chi-Squared P-Value` <- formatC(chi_results$`Chi-Squared P-Value`, format = "e", digits = 3)

# Print the results table
kable(chi_results, caption = "Chi-Squared Tests and Cramér’s V for Key Relationships in the Top 3 Brands")

```










