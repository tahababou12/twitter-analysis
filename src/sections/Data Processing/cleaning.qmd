---
title: "Data Processing"
author: "Taha Ababou"
date: "`r Sys.Date()`"
format: pdf:
editor: 
  markdown: 
    wrap: 72
---

## Data Processing

The data processing phase involved several key steps to prepare the dataset for analysis. Each subsection addresses a specific aspect of data preparation, ensuring the dataset's quality and suitability for subsequent modeling.

```{r setup, include=FALSE}
library(tidytext)
library(dplyr)
library(lubridate)
library(tidyr)
library(tm)
library(ggplot2)
library(kableExtra)
```

```{r}
tweets <- read.csv('../../../data/customer-support-on-twitter/twcs/twcs.csv')
tweets_original <- read.csv('../../../data/customer-support-on-twitter/twcs/twcs.csv')
```

### Data Cleaning

The `inbound` column was converted from character strings to logical format to accurately distinguish between customer and company tweets. The `created_at` column was transformed from character strings to datetime objects using the `lubridate` package in R, enabling precise temporal analysis of tweet timings. Missing values in critical columns (`text`, `inbound`, `created_at`) were identified and handled by removing incomplete records to ensure data integrity. Additionally, duplicate tweets were removed based on the `tweet_id` to prevent redundancy in the analysis.

```{r}
# Convert 'inbound' column to logical
tweets$inbound <- tweets$inbound == "True"

# Convert 'created_at' to datetime
tweets$created_at <- parse_date_time(tweets$created_at, orders = "a b d H:M:S z Y")

# Remove rows with missing critical values
tweets <- tweets %>% drop_na(text, inbound, created_at)

# Remove duplicate tweets
tweets <- tweets %>% distinct(tweet_id, .keep_all = TRUE)
```

```{r}
# Display the first two rows of 'created_at' before and after conversion
comparison_table <- data.frame(
  Before_Conversion = head(tweets_original$created_at, n = 2),
  After_Conversion  = head(tweets$created_at, n = 2)
)

# Display the comparison table using knitr::kable
comparison_table
#kable(comparison_table, caption = "Comparison of `created_at` Before and After Conversion")
```

### Sentiment Analysis

Sentiment analysis was performed on the `text` column to categorize tweets into positive, negative, and neutral sentiments using the `tidytext` package. The sentiment scores were calculated by subtracting the count of negative words from positive words in each tweet. This categorization facilitates the examination of how sentiment influences engagement and response behavior.

```{r}
# Perform sentiment analysis using Bing lexicon
sentiment_scores <- tweets %>%
  unnest_tokens(word, text) %>%
  inner_join(get_sentiments("bing")) %>%
  count(tweet_id, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment_score = positive - negative)

# Merge sentiment scores back to the main dataset
tweets <- tweets %>%
  left_join(sentiment_scores, by = "tweet_id") %>%
  mutate(sentiment_category = case_when(
    sentiment_score > 0 ~ "Positive",
    sentiment_score < 0 ~ "Negative",
    TRUE ~ "Neutral"
  ))
```

### NLP Preprocessing

The `text` data was preprocessed for topic modeling by tokenizing the text, removing stop words, and performing stemming to reduce words to their root forms. This preprocessing ensures that the subsequent topic modeling accurately captures the underlying themes in customer inquiries.

```{r}
#library(tm)

# Create a text corpus
corpus <- Corpus(VectorSource(tweets$text))

# Preprocess the text
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stemDocument)
```

### Feature Engineering

Additional features were engineered to enhance model performance:

-   **Time-Based Features**: Extracted the hour of day, day of week, and whether the tweet was sent during peak hours (9 AM to 5 PM). These features help in understanding temporal patterns in customer support interactions.

    ```{r}
    tweets <- tweets %>%
      mutate(
        hour = hour(created_at),
        day_of_week = wday(created_at, label = TRUE),
        is_peak = if_else(hour >= 9 & hour <= 17, TRUE, FALSE)
      )
    ```

-   **Interaction Features**: Calculated the number of previous interactions by each `author_id` to capture user engagement levels. This metric indicates how active a user is in seeking support, which may influence reply volumes and response times.

    ```{r}
    interaction_counts <- tweets %>%
      group_by(author_id) %>%
      summarise(interaction_count = n())

    tweets <- tweets %>%
      left_join(interaction_counts, by = "author_id")
    ```

-   **Keyword Features**: Extracted keyword-based features from the `text` to capture specific issues or topics that may lead to escalation. This was achieved by identifying the presence of predefined keywords related to common customer issues.

    ```{r}
    #library(tidytext)
    #library(dplyr)

    # Function to extract top N keywords based on TF-IDF
    extract_keywords <- function(data, text_column, top_n = 20) {
      data %>%
        unnest_tokens(word, !!sym(text_column)) %>%
        filter(!word %in% stop_words$word) %>%
        count(word, sort = TRUE) %>%
        anti_join(get_stopwords()) %>%
        filter(n >= 50) %>% # Filter out very rare words
        top_n(top_n, n) %>%
        pull(word)
    }

    # Extract top 20 keywords from the 'text' column
    top_keywords <- extract_keywords(tweets, "text", top_n = 20)
    print(top_keywords)

    # Create binary features for each extracted keyword
    for (word in top_keywords) {
      tweets[[paste0("keyword_", word)]] <- grepl(word, tweets$text, ignore.case = TRUE)
    }

    # Create a composite keyword feature (e.g., count of keyword occurrences)
    tweets$keyword_count <- rowSums(tweets[, paste0("keyword_", top_keywords)])
    ```
