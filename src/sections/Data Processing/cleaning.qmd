---
title: "Data Processing"
author: "Taha Ababou"
date: "`r Sys.Date()`"
format: pdf:
editor: 
  markdown: 
    wrap: 72
---

## Data Processing

The data processing phase involved several key steps to prepare the dataset for analysis. Each subsection addresses a specific aspect of data preparation, ensuring the dataset's quality and suitability for subsequent modeling.

```{r setup, include=FALSE}
library(tidytext)
library(dplyr)
library(lubridate)
library(tidyr)
library(tm)
library(ggplot2)
library(kableExtra)
```

```{r}
tweets <- read.csv('../../../data/customer-support-on-twitter/twcs/twcs.csv')
tweets_original <- read.csv('../../../data/customer-support-on-twitter/twcs/twcs.csv')
```

### Data Cleaning

The `inbound` column was converted from character strings to logical format to accurately distinguish between customer and company tweets. The `created_at` column was transformed from character strings to datetime objects using the `lubridate` package in R, enabling precise temporal analysis of tweet timings. Missing values in critical columns (`text`, `inbound`, `created_at`) were identified and handled by removing incomplete records to ensure data integrity. Additionally, duplicate tweets were removed based on the `tweet_id` to prevent redundancy in the analysis.

```{r}
# Convert 'inbound' column to logical
tweets$inbound <- tweets$inbound == "True"

# Convert 'created_at' to datetime
tweets$created_at <- parse_date_time(tweets$created_at, orders = "a b d H:M:S z Y")

# Remove rows with missing critical values
tweets <- tweets %>% drop_na(text, inbound, created_at)

# Remove duplicate tweets
tweets <- tweets %>% distinct(tweet_id, .keep_all = TRUE)
```

```{r}
# Display the first two rows of 'created_at' before and after conversion
comparison_table <- data.frame(
  Before_Conversion = head(tweets_original$created_at, n = 2),
  After_Conversion  = head(tweets$created_at, n = 2)
)

# Display the comparison table using knitr::kable
comparison_table
#kable(comparison_table, caption = "Comparison of `created_at` Before and After Conversion")
```

### Sentiment Analysis

Sentiment analysis was performed on the `text` column to categorize tweets into **positive**, **negative**, and **neutral** sentiments using the `tidytext` package. The sentiment scores were calculated by subtracting the count of negative words from positive words in each tweet. This categorization facilitates the examination of how sentiment influences engagement and response behavior.

*To eliminate the many-to-many join warning, we ensure that the sentiment lexicon contains unique entries for each word before performing the join operation.*

```{r sentiment_analysis, echo=TRUE}
library(tidytext)
library(tidyr)
library(dplyr)
library(knitr)

# Step 1: Tokenize the text and preprocess
tweet_words <- tweets %>%
  unnest_tokens(word, text)

# Step 2: Ensure uniqueness in the Bing sentiment lexicon
bing_sentiments_unique <- get_sentiments("bing") %>%
  distinct(word, .keep_all = TRUE)

# Optional: Check for duplicates in the sentiment lexicon
duplicate_bing <- get_sentiments("bing") %>%
  group_by(word) %>%
  summarise(count = n()) %>%
  filter(count > 1)

if(nrow(duplicate_bing) > 0){
  warning("There are duplicate entries in the Bing sentiment lexicon. They will be removed to ensure uniqueness.")
}

# Step 3: Perform the inner join with the unique sentiment lexicon
sentiment_scores <- tweet_words %>%
  inner_join(bing_sentiments_unique, by = "word") %>%
  count(tweet_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment_score = positive - negative)

# Step 4: Merge sentiment scores back to the main dataset
tweets <- tweets %>%
  left_join(sentiment_scores, by = "tweet_id") %>%
  mutate(sentiment_category = case_when(
    sentiment_score > 0 ~ "Positive",
    sentiment_score < 0 ~ "Negative",
    TRUE ~ "Neutral"
  ))

# Display the first two rows of sentiment_scores for verification
head(sentiment_scores, n = 2)
```

```{r}
# [Graph: Sentiment Scores for First Two Tweets]

library(ggplot2)
library(dplyr)

# Count the number of tweets in each sentiment category
sentiment_counts <- tweets %>%
  group_by(sentiment_category) %>%
  summarise(count = n())

# Create a bar plot
ggplot(sentiment_counts, aes(x = sentiment_category, y = count, fill = sentiment_category)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("Positive" = "steelblue", "Negative" = "salmon", "Neutral" = "grey")) +
  labs(title = "Distribution of Sentiment Categories",
       x = "Sentiment Category",
       y = "Number of Tweets") +
  theme_minimal() +
  theme(legend.position = "none")


```
### NLP Preprocessing

The `text` data was preprocessed for topic modeling by tokenizing the text, removing stop words, and performing stemming to reduce words to their root forms. This preprocessing ensures that the subsequent topic modeling accurately captures the underlying themes in customer inquiries.

```{r}
#library(tm)

# Create a text corpus
corpus <- Corpus(VectorSource(tweets$text))

# Preprocess the text
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stemDocument)
```

### Feature Engineering

Additional features were engineered to enhance model performance:

-   **Time-Based Features**: Extracted the hour of day, day of week, and whether the tweet was sent during peak hours (9 AM to 5 PM). These features help in understanding temporal patterns in customer support interactions.

```{r}
# Count the number of tweets per hour
tweets_by_hour <- tweets %>%
  group_by(hour) %>%
  summarise(count = n())

# Plot the distribution
ggplot(tweets_by_hour, aes(x = hour, y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Tweets by Hour of Day",
       x = "Hour of Day",
       y = "Number of Tweets") +
  scale_x_continuous(breaks = 0:23) +
  theme_minimal()
```

```{r}
# Count the number of tweets per day of the week
tweets_by_day <- tweets %>%
  group_by(day_of_week) %>%
  summarise(count = n())

# Plot the distribution
ggplot(tweets_by_day, aes(x = day_of_week, y = count, fill = day_of_week)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Pastel1") +
  labs(title = "Number of Tweets by Day of the Week",
       x = "Day of the Week",
       y = "Number of Tweets") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
# Count the number of tweets during peak and non-peak hours
tweets_peak_nonpeak <- tweets %>%
  group_by(is_peak) %>%
  summarise(count = n())

# Convert is_peak to a factor with descriptive labels
tweets_peak_nonpeak$is_peak <- factor(tweets_peak_nonpeak$is_peak, 
                                     levels = c(TRUE, FALSE),
                                     labels = c("Peak Hours (9 AM - 5 PM)", "Non-Peak Hours"))

# Plot the comparison
ggplot(tweets_peak_nonpeak, aes(x = is_peak, y = count, fill = is_peak)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("Peak Hours (9 AM - 5 PM)" = "orange", "Non-Peak Hours" = "purple")) +
  labs(title = "Number of Tweets During Peak vs. Non-Peak Hours",
       x = "Time Period",
       y = "Number of Tweets") +
  theme_minimal() +
  theme(legend.position = "none")
```
-   **Interaction Features**: Calculated the number of previous interactions by each `author_id` to capture user engagement levels. This metric indicates how active a user is in seeking support, which may influence reply volumes and response times.

```{r}
interaction_counts <- tweets %>%
  group_by(author_id) %>%
  summarise(interaction_count = n())

tweets <- tweets %>%
  left_join(interaction_counts, by = "author_id")
```

```{r}
# Plot histogram of interaction counts
ggplot(tweets, aes(x = interaction_count)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Interaction Counts",
       x = "Number of Previous Interactions",
       y = "Frequency") +
  theme_minimal()
```

-   **Keyword Features**: Extracted keyword-based features from the `text` to capture specific issues or topics that may lead to escalation. This was achieved by identifying the presence of predefined keywords related to common customer issues.

```{r}
# Function to extract top N keywords based on TF-IDF
extract_keywords <- function(data, text_column, top_n = 20) {
  data %>%
    unnest_tokens(word, !!sym(text_column)) %>%
    filter(!word %in% stop_words$word) %>%
    count(word, sort = TRUE) %>%
    anti_join(get_stopwords()) %>%
    filter(n >= 50) %>% # Filter out very rare words
    top_n(top_n, n) %>%
    pull(word)
}

# Extract top 20 keywords from the 'text' column
top_keywords <- extract_keywords(tweets, "text", top_n = 20)
print(top_keywords)

# Create binary features for each extracted keyword
for (word in top_keywords) {
  tweets[[paste0("keyword_", word)]] <- grepl(word, tweets$text, ignore.case = TRUE)
}

# Create a composite keyword feature (e.g., count of keyword occurrences)
tweets$keyword_count <- rowSums(tweets[, paste0("keyword_", top_keywords)])
```
